# About Goodfire

Behind our name: Like fire, AI holds the potential for both immense benefit and significant risk. Just as mastering fire transformed human history, we believe the safe and intentional development of AI will shape the future of our species. Our goal is to tame this new fire.

Goodfire is an AI interpretability research company focused on understanding and designing AI systems that people can trust. Our mission is to advance humanity's understanding of AI to build safe and powerful AI systems. We believe that deep research breakthroughs are necessary to make this possible.

Goodfire is a public benefit corporation headquartered in San Francisco with a team of the world’s top interpretability researchers and engineers from organizations like OpenAI and DeepMind. We’ve raised $59M from investors like Menlo, Lightspeed and Anthropic and work with customers including Arc Institute, Mayo Clinic, and Rakuten.

# About the role

We’re looking for a Research Engineer to help build the systems that power cutting-edge interpretability research. You’ll play a central role in scaling and deploying tools that analyze and steer large models.

## Where you might contribute:

- Interpretability mechanisms – Build tools and infra to support foundational model probing.
- Moonshots – Rapidly prototype novel interpretability systems with high-upside potential.
- Applied research & usability – Turn interpretability techniques into robust, usable features.

We'll work with you to determine the pod that best aligns with your strengths.

## Key Responsibilities:

- Develop robust tooling for analyzing and visualizing model internals.
- Optimize pipelines and infra for large-scale interpretability workflows.
- Partner with researchers to iterate quickly on experimental techniques.
- Help deploy interpretability tools into product and production contexts.
- Ensure system reliability, reproducibility, and performance.

## What you’ll bring

### Required experience

- 5+ years of experience in ML infra, research engineering, or systems programming.
- Comfort working across research and engineering boundaries.
- Expertise in Python, PyTorch or Jax, and distributed systems.
- Experience deploying and maintaining ML systems at scale.
- Enthusiasm for helping AI systems become safer and more interpretable.

### Preferred qualifications

- Prior work on model internals, explainability, or interpretability.
- Open-source ML infra contributions.
- Startup or lab experience in fast-moving teams.

# Our values

Goodfire is looking for individuals who embody our values and share our deep commitment to making interpretability accessible. We are building a team first and foremost.

## Put mission and team first

All we do is in service of our mission. We trust each other, deeply care about the success of the organization, and choose to put our team above ourselves.

## Improve constantly

We are constantly looking to improve every piece of the business. We proactively critique ourselves and others in a kind and thoughtful way that translates to practical improvements in the organization. We are pragmatic and consistently implement the obvious fixes that work.

## Take ownership and initiative

There are no bystanders here. We proactively identify problems and take full responsibility over getting a strong result. We are self-driven, own our mistakes, and feel deep responsibility over what we’re building.

## Action today

We have a small amount of time to do something incredibly hard and meaningful. The pace and intensity of the organization is high. If we can take action today or tomorrow, we will choose to do it today.
